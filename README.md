# ğŸ§  ê¸°ì´ˆë¶€í„° ì‹œì‘í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ëª¨ë¸ (PyTorch ë²„ì „)

## íŠœí† ë¦¬ì–¼ ëª©ë¡

### ê¸°ì´ˆë¶€í„° ì‹œì‘í•˜ëŠ” NLP ì‹œë¦¬ì¦ˆ with. íŒŒì´í† ì¹˜(PyTorch) í•œêµ­ ì‚¬ìš©ìëª¨ì„

- [NLP 1: í…ìŠ¤íŠ¸ ì „ì²´ ì²˜ë¦¬ì™€ ì›Œë“œ ì„ë²¤ë“œ](https://johyeongseob.tistory.com/49)  
- [NLP 2: ìˆœíšŒ ì‹œê°„ë ¥ë²” (RNN, LSTM, GRU)](https://johyeongseob.tistory.com/50)  
- [NLP 3: ì–´í…ì…˜ê³¼ Seq2Seq ëª¨ë¸](https://johyeongseob.tistory.com/51)  

### ì¸ê³µì§€ëŠ¥ ëŒ€í‘œ ëª¨ë¸

- [Transformer (Vaswani et al., 2017)](https://johyeongseob.tistory.com/53)  
  - ë…¼ë¬¸: *Attention Is All You Need*  
- [Vision Transformer (ViT) (Dosovitskiy et al., 2020)](https://johyeongseob.tistory.com/71)  
  - ë…¼ë¬¸: *An Image is Worth 16x16 Words*  
- [CLIP (Radford et al., 2021)](https://johyeongseob.tistory.com/72)  
  - ë…¼ë¬¸: *Learning Transferable Visual Models from Natural Language Supervision*  

## ì°¸ê³  ë…¼ë¬¸

1. Vaswani et al., â€œAttention is All You Needâ€, NeurIPS 2017.  
2. Dosovitskiy et al., â€œAn Image is Worth 16x16 Wordsâ€, arXiv 2020.  
3. Radford et al., â€œLearning Transferable Visual Models from Natural Language Supervisionâ€, ICML 2021.

